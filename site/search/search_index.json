{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to french amendments analysis project We use poetry . Commands poetry install - Install depedencies poetry shell - Start a new shell inside the venv. poetry run python <python_name> - Run a python file train_lm - Train the language model","title":"Welcome to french amendments analysis project"},{"location":"index.html#welcome-to-french-amendments-analysis-project","text":"We use poetry .","title":"Welcome to french amendments analysis project"},{"location":"index.html#commands","text":"poetry install - Install depedencies poetry shell - Start a new shell inside the venv. poetry run python <python_name> - Run a python file train_lm - Train the language model","title":"Commands"},{"location":"domain/clusters_finder-reference.html","text":"clusters_finder Get sentences embeddings and generate cluster according to the number of cluster previously defined. An UMAP dimension reduction and a Kmenoid with cosine distance are performed for this task. Attributes: Name Type Description sentence_embeddings numpy array) sentence embeddings models property readonly Get sentences embeddings and generate cluster according to the number of cluster previously defined. An UMAP dimension reduction and a Kmenoid with cosine distance are performed for this task. Returns: Type Description sklearn.model fitted umap model with attribute 'sentence_embeddings' Returns: Type Description sklearn.model fitted kmenoid model from umaped 'sentence_embeddings' __init__ ( self , sentence_embeddings , n_umap = 15 , n_clusters = 13 ) special Class initilization Parameters: Name Type Description Default sentence_embedding numpy.Array numpy array of previous Bert embedding required n_clusters list number of topics to split the dataset 13 n_umap list number of axis after UMAP size reduction 15 Source code in amendements_analysis/domain/clusters_finder.py def __init__ ( self , sentence_embeddings , n_umap = 15 , n_clusters = 13 ): \"\"\"Class initilization Parameters: sentence_embedding numpy.Array : numpy array of previous Bert embedding n_clusters list : number of topics to split the dataset n_umap list : number of axis after UMAP size reduction \"\"\" self . sentence_embeddings = sentence_embeddings self . n_clusters = n_clusters self . n_umap = n_umap","title":"Cluster Finder"},{"location":"domain/clusters_finder-reference.html#amendements_analysis.domain.clusters_finder","text":"","title":"amendements_analysis.domain.clusters_finder"},{"location":"domain/clusters_finder-reference.html#amendements_analysis.domain.clusters_finder.clusters_finder","text":"Get sentences embeddings and generate cluster according to the number of cluster previously defined. An UMAP dimension reduction and a Kmenoid with cosine distance are performed for this task. Attributes: Name Type Description sentence_embeddings numpy array) sentence embeddings","title":"clusters_finder"},{"location":"domain/clusters_finder-reference.html#amendements_analysis.domain.clusters_finder.clusters_finder.models","text":"Get sentences embeddings and generate cluster according to the number of cluster previously defined. An UMAP dimension reduction and a Kmenoid with cosine distance are performed for this task. Returns: Type Description sklearn.model fitted umap model with attribute 'sentence_embeddings' Returns: Type Description sklearn.model fitted kmenoid model from umaped 'sentence_embeddings'","title":"models"},{"location":"domain/clusters_finder-reference.html#amendements_analysis.domain.clusters_finder.clusters_finder.__init__","text":"Class initilization Parameters: Name Type Description Default sentence_embedding numpy.Array numpy array of previous Bert embedding required n_clusters list number of topics to split the dataset 13 n_umap list number of axis after UMAP size reduction 15 Source code in amendements_analysis/domain/clusters_finder.py def __init__ ( self , sentence_embeddings , n_umap = 15 , n_clusters = 13 ): \"\"\"Class initilization Parameters: sentence_embedding numpy.Array : numpy array of previous Bert embedding n_clusters list : number of topics to split the dataset n_umap list : number of axis after UMAP size reduction \"\"\" self . sentence_embeddings = sentence_embeddings self . n_clusters = n_clusters self . n_umap = n_umap","title":"__init__()"},{"location":"domain/lda_model-reference.html","text":"LatentDirichletAllocationModel Performs a Latent Dirichlet model to a list of amendments Attributes: Name Type Description amendments_list list list of all amendments lda_model property readonly Property to perform the LDA Returns: Type Description lda_model sklearn.decomposition.LatentDirichletAllocation Model of LDA __init__ ( self , amendments_list ) special Class initilization Parameters: Name Type Description Default amendments_list list list of all amendments required Source code in amendements_analysis/domain/lda_model.py def __init__ ( self , amendments_list ): \"\"\" Class initilization Parameters: amendments_list list: list of all amendments \"\"\" self . amendments_list = amendments_list","title":"LDA Model"},{"location":"domain/lda_model-reference.html#amendements_analysis.domain.lda_model","text":"","title":"amendements_analysis.domain.lda_model"},{"location":"domain/lda_model-reference.html#amendements_analysis.domain.lda_model.LatentDirichletAllocationModel","text":"Performs a Latent Dirichlet model to a list of amendments Attributes: Name Type Description amendments_list list list of all amendments","title":"LatentDirichletAllocationModel"},{"location":"domain/lda_model-reference.html#amendements_analysis.domain.lda_model.LatentDirichletAllocationModel.lda_model","text":"Property to perform the LDA Returns: Type Description lda_model sklearn.decomposition.LatentDirichletAllocation Model of LDA","title":"lda_model"},{"location":"domain/lda_model-reference.html#amendements_analysis.domain.lda_model.LatentDirichletAllocationModel.__init__","text":"Class initilization Parameters: Name Type Description Default amendments_list list list of all amendments required Source code in amendements_analysis/domain/lda_model.py def __init__ ( self , amendments_list ): \"\"\" Class initilization Parameters: amendments_list list: list of all amendments \"\"\" self . amendments_list = amendments_list","title":"__init__()"},{"location":"domain/lm_training-reference.html","text":"LM_Trainer LM_Trainer Train the language model of camembert model from huggingface __init__ ( self ) special Class initialisation Source code in amendements_analysis/domain/lm_training.py def __init__ ( self ): \"\"\"Class initialisation\"\"\" self . tokenizer = CamembertTokenizerFast . from_pretrained ( \"camembert-base\" , max_len = 512 ) self . model = CamembertForMaskedLM . from_pretrained ( \"camembert-base\" ) self . training_args = TrainingArguments ( output_dir = stg . TMP_MODEL_DIR , overwrite_output_dir = True , # num_train_epochs=3, # per_device_train_batch_size=16, # fp16=True, save_steps = 10_000 , save_total_limit = 2 , ) train ( self ) Train language model of camembert-base from huggingface Returns: Type Description transformers.model Trained Model Source code in amendements_analysis/domain/lm_training.py def train ( self ): \"\"\" Train language model of camembert-base from **huggingface** Returns: transformers.model: Trained Model \"\"\" datasets , column_names = self . _create_datasets () tokenized_datasets = datasets . map ( self . _tokenize_function , batched = True ) data_collator = DataCollatorForLanguageModeling ( tokenizer = self . tokenizer , mlm = True , mlm_probability = 0.15 ) lm_trainer = Trainer ( model = self . model , args = self . training_args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], tokenizer = self . tokenizer , data_collator = data_collator , ) lm_trainer . train () return lm_trainer","title":"Run Training Language model"},{"location":"domain/lm_training-reference.html#amendements_analysis.domain.lm_training","text":"","title":"amendements_analysis.domain.lm_training"},{"location":"domain/lm_training-reference.html#amendements_analysis.domain.lm_training.LM_Trainer","text":"LM_Trainer Train the language model of camembert model from huggingface","title":"LM_Trainer"},{"location":"domain/lm_training-reference.html#amendements_analysis.domain.lm_training.LM_Trainer.__init__","text":"Class initialisation Source code in amendements_analysis/domain/lm_training.py def __init__ ( self ): \"\"\"Class initialisation\"\"\" self . tokenizer = CamembertTokenizerFast . from_pretrained ( \"camembert-base\" , max_len = 512 ) self . model = CamembertForMaskedLM . from_pretrained ( \"camembert-base\" ) self . training_args = TrainingArguments ( output_dir = stg . TMP_MODEL_DIR , overwrite_output_dir = True , # num_train_epochs=3, # per_device_train_batch_size=16, # fp16=True, save_steps = 10_000 , save_total_limit = 2 , )","title":"__init__()"},{"location":"domain/lm_training-reference.html#amendements_analysis.domain.lm_training.LM_Trainer.train","text":"Train language model of camembert-base from huggingface Returns: Type Description transformers.model Trained Model Source code in amendements_analysis/domain/lm_training.py def train ( self ): \"\"\" Train language model of camembert-base from **huggingface** Returns: transformers.model: Trained Model \"\"\" datasets , column_names = self . _create_datasets () tokenized_datasets = datasets . map ( self . _tokenize_function , batched = True ) data_collator = DataCollatorForLanguageModeling ( tokenizer = self . tokenizer , mlm = True , mlm_probability = 0.15 ) lm_trainer = Trainer ( model = self . model , args = self . training_args , train_dataset = tokenized_datasets [ \"train\" ], eval_dataset = tokenized_datasets [ \"validation\" ], tokenizer = self . tokenizer , data_collator = data_collator , ) lm_trainer . train () return lm_trainer","title":"train()"},{"location":"domain/topic_finder-reference.html","text":"TextCleaner Get amendments df and perform pre processing compatible with Topics words finder This includes : remove accent lemmatize specific regex flag for specific terms Attributes: Name Type Description df pandas.DataFrame dataframe to clean Returns: Type Description pandas.DataFrame df_cleaned __init__ ( self , flag_dict ) special Class initilization Parameters: Name Type Description Default flag_dict dict dict of regex to replace by flag required Source code in amendements_analysis/domain/topic_finder.py def __init__ ( self , flag_dict ): \"\"\"Class initilization Parameters: flag_dict dict: dict of regex to replace by flag \"\"\" self . flag_dict = flag_dict flag_text ( text , flag_dict ) staticmethod Use Regex to sub on text Parameters: Name Type Description Default text str Text to check required flag_dict str regex expression required Returns: Type Description str texte after regex Source code in amendements_analysis/domain/topic_finder.py @staticmethod def flag_text ( text , flag_dict ): \"\"\"Use Regex to sub on text Args: text (str): Text to check flag_dict (str): regex expression Returns: str: texte after regex \"\"\" for regex , flag in flag_dict . items (): text = re . sub ( regex , flag , str ( text )) return text lemmatizer ( df ) staticmethod Lemmatize dataframe Series Parameters: Name Type Description Default df Series list to lemmatize required Returns: Type Description Series lemmatized list Source code in amendements_analysis/domain/topic_finder.py @staticmethod def lemmatizer ( df ): \"\"\"Lemmatize dataframe Series Args: df (Series): list to lemmatize Returns: Series: lemmatized list \"\"\" df_tmp = [] nlp = fr_core_news_md . load ( disable = [ \"ner\" , \"parser\" ]) nlp . add_pipe ( nlp . create_pipe ( \"sentencizer\" )) for doc in nlp . pipe ( list ( df ), batch_size = 2000 ): texts = [ ent . lemma_ for ent in doc ] tmp = \" \" . join ( texts ) df_tmp . append ( tmp ) # doc = nlp.pipe(text) # text_lemm = [token.lemma_ for token in doc] # text_lemm = \" \".join(text_lemm) return pd . DataFrame ( df_tmp )[ 0 ] remove_accents ( text , method = 'unicodedata' ) staticmethod Remove accents from text Parameters: Name Type Description Default text str text to clean required method str method to remove accents. Defaults to \"unicodedata\". 'unicodedata' Exceptions: Type Description ValueError Method not supported Returns: Type Description str string without accents Source code in amendements_analysis/domain/topic_finder.py @staticmethod def remove_accents ( text , method = \"unicodedata\" ): \"\"\"Remove accents from text Args: text (str): text to clean method (str, optional): method to remove accents. Defaults to \"unicodedata\". Raises: ValueError: Method not supported Returns: str: string without accents \"\"\" if method == \"unidecode\" : return unidecode . unidecode ( text ) elif method == \"unicodedata\" : utf8_str = ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ASCII\" , \"ignore\" ) . decode ( \"utf-8\" ) ) return utf8_str else : raise ValueError ( \"Possible values for method are 'unicodedata' or 'unidecode'\" ) transform ( self , df ) Clean amendments datasets Parameters: Name Type Description Default df pandas.DataFrame Dataframe with amendments to clean required Returns: Type Description pandas.DataFrame Cleaned Dataframe Source code in amendements_analysis/domain/topic_finder.py def transform ( self , df ): \"\"\"Clean amendments datasets Args: df (pandas.DataFrame): Dataframe with amendments to clean Returns: pandas.DataFrame: Cleaned Dataframe \"\"\" df_cleaned = df . copy () df_cleaned = ( df_cleaned [ \"expose_sommaire\" ] if isinstance ( df_cleaned , pd . DataFrame ) else df_cleaned ) df_cleaned = df_cleaned . apply ( lambda x : str ( x )) df_cleaned = self . lemmatizer ( df_cleaned ) # df_cleaned = df_cleaned.apply(self.lowercase) df_cleaned = df_cleaned . apply ( self . flag_text , args = ( self . flag_dict ,)) return df_cleaned TopicWordsFinder Get amendements df with labels from model and perform clustering words recognition Attributes: Name Type Description df pandas.DataFrame dataframe words_per_topic property readonly Property to get the numbers of words per topic Returns: Type Description list top of n words list: size of topics __init__ ( self , df ) special Class initilization Parameters: Name Type Description Default df pandas.DataFrame cleaned and lemmatized dataframe with topic already assigned from previous clusterization required Source code in amendements_analysis/domain/topic_finder.py def __init__ ( self , df ): \"\"\"Class initilization Args: df (pandas.DataFrame): cleaned and lemmatized dataframe with topic already assigned from previous clusterization \"\"\" self . df = df","title":"Topic Finder"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder","text":"","title":"amendements_analysis.domain.topic_finder"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TextCleaner","text":"Get amendments df and perform pre processing compatible with Topics words finder This includes : remove accent lemmatize specific regex flag for specific terms Attributes: Name Type Description df pandas.DataFrame dataframe to clean Returns: Type Description pandas.DataFrame df_cleaned","title":"TextCleaner"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TextCleaner.__init__","text":"Class initilization Parameters: Name Type Description Default flag_dict dict dict of regex to replace by flag required Source code in amendements_analysis/domain/topic_finder.py def __init__ ( self , flag_dict ): \"\"\"Class initilization Parameters: flag_dict dict: dict of regex to replace by flag \"\"\" self . flag_dict = flag_dict","title":"__init__()"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TextCleaner.flag_text","text":"Use Regex to sub on text Parameters: Name Type Description Default text str Text to check required flag_dict str regex expression required Returns: Type Description str texte after regex Source code in amendements_analysis/domain/topic_finder.py @staticmethod def flag_text ( text , flag_dict ): \"\"\"Use Regex to sub on text Args: text (str): Text to check flag_dict (str): regex expression Returns: str: texte after regex \"\"\" for regex , flag in flag_dict . items (): text = re . sub ( regex , flag , str ( text )) return text","title":"flag_text()"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TextCleaner.lemmatizer","text":"Lemmatize dataframe Series Parameters: Name Type Description Default df Series list to lemmatize required Returns: Type Description Series lemmatized list Source code in amendements_analysis/domain/topic_finder.py @staticmethod def lemmatizer ( df ): \"\"\"Lemmatize dataframe Series Args: df (Series): list to lemmatize Returns: Series: lemmatized list \"\"\" df_tmp = [] nlp = fr_core_news_md . load ( disable = [ \"ner\" , \"parser\" ]) nlp . add_pipe ( nlp . create_pipe ( \"sentencizer\" )) for doc in nlp . pipe ( list ( df ), batch_size = 2000 ): texts = [ ent . lemma_ for ent in doc ] tmp = \" \" . join ( texts ) df_tmp . append ( tmp ) # doc = nlp.pipe(text) # text_lemm = [token.lemma_ for token in doc] # text_lemm = \" \".join(text_lemm) return pd . DataFrame ( df_tmp )[ 0 ]","title":"lemmatizer()"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TextCleaner.remove_accents","text":"Remove accents from text Parameters: Name Type Description Default text str text to clean required method str method to remove accents. Defaults to \"unicodedata\". 'unicodedata' Exceptions: Type Description ValueError Method not supported Returns: Type Description str string without accents Source code in amendements_analysis/domain/topic_finder.py @staticmethod def remove_accents ( text , method = \"unicodedata\" ): \"\"\"Remove accents from text Args: text (str): text to clean method (str, optional): method to remove accents. Defaults to \"unicodedata\". Raises: ValueError: Method not supported Returns: str: string without accents \"\"\" if method == \"unidecode\" : return unidecode . unidecode ( text ) elif method == \"unicodedata\" : utf8_str = ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ASCII\" , \"ignore\" ) . decode ( \"utf-8\" ) ) return utf8_str else : raise ValueError ( \"Possible values for method are 'unicodedata' or 'unidecode'\" )","title":"remove_accents()"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TextCleaner.transform","text":"Clean amendments datasets Parameters: Name Type Description Default df pandas.DataFrame Dataframe with amendments to clean required Returns: Type Description pandas.DataFrame Cleaned Dataframe Source code in amendements_analysis/domain/topic_finder.py def transform ( self , df ): \"\"\"Clean amendments datasets Args: df (pandas.DataFrame): Dataframe with amendments to clean Returns: pandas.DataFrame: Cleaned Dataframe \"\"\" df_cleaned = df . copy () df_cleaned = ( df_cleaned [ \"expose_sommaire\" ] if isinstance ( df_cleaned , pd . DataFrame ) else df_cleaned ) df_cleaned = df_cleaned . apply ( lambda x : str ( x )) df_cleaned = self . lemmatizer ( df_cleaned ) # df_cleaned = df_cleaned.apply(self.lowercase) df_cleaned = df_cleaned . apply ( self . flag_text , args = ( self . flag_dict ,)) return df_cleaned","title":"transform()"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TopicWordsFinder","text":"Get amendements df with labels from model and perform clustering words recognition Attributes: Name Type Description df pandas.DataFrame dataframe","title":"TopicWordsFinder"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TopicWordsFinder.words_per_topic","text":"Property to get the numbers of words per topic Returns: Type Description list top of n words list: size of topics","title":"words_per_topic"},{"location":"domain/topic_finder-reference.html#amendements_analysis.domain.topic_finder.TopicWordsFinder.__init__","text":"Class initilization Parameters: Name Type Description Default df pandas.DataFrame cleaned and lemmatized dataframe with topic already assigned from previous clusterization required Source code in amendements_analysis/domain/topic_finder.py def __init__ ( self , df ): \"\"\"Class initilization Args: df (pandas.DataFrame): cleaned and lemmatized dataframe with topic already assigned from previous clusterization \"\"\" self . df = df","title":"__init__()"},{"location":"domain/topic_predicter-reference.html","text":"topic_predicter Get amendement and attribute a topic. Attributes: Name Type Description df pandas.DataFrame pandas DataFrame with umap_model umap.model model to reduce dimension kmenoid_model_fitted sklearn.model model predicted_topic property readonly Property to get the predicted topic Returns: Type Description int, str predicted number of topic and topic name __init__ ( self , sentence_embedding , umap_model_fit , cluster_model_fit ) special Class initilization Parameters: Name Type Description Default sentence_embedding numpy.Array numpy array of previous Bert embedding required umap_model_fit umap.model fit model for umap dimension reduction required cluster_model_fit cluster.model fit model for clustering prediction required Source code in amendements_analysis/domain/topic_predicter.py def __init__ ( self , sentence_embedding , umap_model_fit , cluster_model_fit ): \"\"\"Class initilization Parameters: sentence_embedding numpy.Array: numpy array of previous Bert embedding umap_model_fit umap.model : fit model for umap dimension reduction cluster_model_fit cluster.model : fit model for clustering prediction \"\"\" self . sentence_embedding = sentence_embedding self . cluster_model_fit = cluster_model_fit self . umap_model_fit = umap_model_fit","title":"Topic Predicter"},{"location":"domain/topic_predicter-reference.html#amendements_analysis.domain.topic_predicter","text":"","title":"amendements_analysis.domain.topic_predicter"},{"location":"domain/topic_predicter-reference.html#amendements_analysis.domain.topic_predicter.topic_predicter","text":"Get amendement and attribute a topic. Attributes: Name Type Description df pandas.DataFrame pandas DataFrame with umap_model umap.model model to reduce dimension kmenoid_model_fitted sklearn.model model","title":"topic_predicter"},{"location":"domain/topic_predicter-reference.html#amendements_analysis.domain.topic_predicter.topic_predicter.predicted_topic","text":"Property to get the predicted topic Returns: Type Description int, str predicted number of topic and topic name","title":"predicted_topic"},{"location":"domain/topic_predicter-reference.html#amendements_analysis.domain.topic_predicter.topic_predicter.__init__","text":"Class initilization Parameters: Name Type Description Default sentence_embedding numpy.Array numpy array of previous Bert embedding required umap_model_fit umap.model fit model for umap dimension reduction required cluster_model_fit cluster.model fit model for clustering prediction required Source code in amendements_analysis/domain/topic_predicter.py def __init__ ( self , sentence_embedding , umap_model_fit , cluster_model_fit ): \"\"\"Class initilization Parameters: sentence_embedding numpy.Array: numpy array of previous Bert embedding umap_model_fit umap.model : fit model for umap dimension reduction cluster_model_fit cluster.model : fit model for clustering prediction \"\"\" self . sentence_embedding = sentence_embedding self . cluster_model_fit = cluster_model_fit self . umap_model_fit = umap_model_fit","title":"__init__()"},{"location":"infrastructure/building_dataset-reference.html","text":"Get amendments from the Web if csv file or zip file doesn't exist Attributes is_split_sentence: bool rewrite_csv: bool Properties data: pandas.DataFrame data property readonly Property to get the Dataframe, If csv exists => use it unless If Zip exists => use it unless download Data from Web Returns pandas.DataFrame dataframe witl all amendments __init__ ( self , is_split_sentence = False , rewrite_csv = False , get_only_amendments = True ) special Class initilization Parameters is_split_sentence : bool, optional set True if you want the amendment splitted in sentences, by default True rewrite_csv : bool, optional Set True if you want to overwrite csv from json files, by default False Source code in amendements_analysis/infrastructure/building_dataset.py def __init__ ( self , is_split_sentence = False , rewrite_csv = False , get_only_amendments = True ): \"\"\"Class initilization Parameters ---------- is_split_sentence : bool, optional set True if you want the amendment splitted in sentences, by default True rewrite_csv : bool, optional Set True if you want to overwrite csv from json files, by default False \"\"\" self . is_split_sentence = is_split_sentence self . rewrite_csv = rewrite_csv self . get_only_amendments = get_only_amendments cleanString ( text ) staticmethod Clean a string by unescape html char and tags Parameters text : str str to clean Returns str Cleaned str Source code in amendements_analysis/infrastructure/building_dataset.py @staticmethod def cleanString ( text ): \"\"\"Clean a string by unescape html char and tags Parameters ---------- text : str str to clean Returns ------- str Cleaned str \"\"\" text_unescaped = html . unescape ( text ) p = re . compile ( r \"<.*?>\" ) return p . sub ( \"\" , text_unescaped ) create_train_test_files ( self , df , force_write_csv = False ) Split dataframe into train and test to write it in order to read it with the transformer Parameters df : pandas.DataFrame Dataframe we want to split Returns pandas.DataFrame Two new dataframes got by splitting df Source code in amendements_analysis/infrastructure/building_dataset.py def create_train_test_files ( self , df , force_write_csv = False ): \"\"\"Split dataframe into train and test to write it in order to read it with the transformer Parameters ---------- df : pandas.DataFrame Dataframe we want to split Returns ------- pandas.DataFrame Two new dataframes got by splitting df \"\"\" train_filename = os . path . join ( stg . INTERIM_DIR , stg . CSV_DATA_TRAIN_LM ) test_filename = os . path . join ( stg . INTERIM_DIR , stg . CSV_DATA_TEST_LM ) if force_write_csv or not self . check_files_exists ( train_filename , test_filename ): dataset = df [ \"expose_sommaire\" ] if isinstance ( df , pd . DataFrame ) else df X_train , X_test = train_test_split ( dataset , test_size = 0.2 , random_state = stg . RANDOM_STATE ) X_train . to_csv ( train_filename , index = False ) X_test . to_csv ( test_filename , index = False ) print ( f \"Files X_train & X_test wrote in { stg . INTERIM_DIR } \" ) else : X_train = pd . read_csv ( train_filename ) X_test = pd . read_csv ( test_filename ) return X_train , X_test split_text_into_sentences ( data ) staticmethod Method to split a text into sentences Parameters data : str text to split Returns list list of sentences got by the split Source code in amendements_analysis/infrastructure/building_dataset.py @staticmethod def split_text_into_sentences ( data ): \"\"\"Method to split a text into sentences Parameters ---------- data : str text to split Returns ------- list list of sentences got by the split \"\"\" reg = re . compile ( r \"(?<!\\b[M]|\\b[Dr])[.?!]\\s*(?=[A-Z])\" ) return reg . split ( data )","title":"Dataset Builder"},{"location":"infrastructure/building_dataset-reference.html#amendements_analysis.infrastructure.building_dataset.DatasetBuilder","text":"Get amendments from the Web if csv file or zip file doesn't exist","title":"amendements_analysis.infrastructure.building_dataset.DatasetBuilder"},{"location":"infrastructure/building_dataset-reference.html#attributes","text":"is_split_sentence: bool rewrite_csv: bool","title":"Attributes"},{"location":"infrastructure/building_dataset-reference.html#properties","text":"data: pandas.DataFrame","title":"Properties"},{"location":"infrastructure/building_dataset-reference.html#amendements_analysis.infrastructure.building_dataset.DatasetBuilder.data","text":"Property to get the Dataframe, If csv exists => use it unless If Zip exists => use it unless download Data from Web","title":"data"},{"location":"infrastructure/building_dataset-reference.html#returns","text":"pandas.DataFrame dataframe witl all amendments","title":"Returns"},{"location":"infrastructure/building_dataset-reference.html#amendements_analysis.infrastructure.building_dataset.DatasetBuilder.__init__","text":"Class initilization","title":"__init__()"},{"location":"infrastructure/building_dataset-reference.html#parameters","text":"is_split_sentence : bool, optional set True if you want the amendment splitted in sentences, by default True rewrite_csv : bool, optional Set True if you want to overwrite csv from json files, by default False Source code in amendements_analysis/infrastructure/building_dataset.py def __init__ ( self , is_split_sentence = False , rewrite_csv = False , get_only_amendments = True ): \"\"\"Class initilization Parameters ---------- is_split_sentence : bool, optional set True if you want the amendment splitted in sentences, by default True rewrite_csv : bool, optional Set True if you want to overwrite csv from json files, by default False \"\"\" self . is_split_sentence = is_split_sentence self . rewrite_csv = rewrite_csv self . get_only_amendments = get_only_amendments","title":"Parameters"},{"location":"infrastructure/building_dataset-reference.html#amendements_analysis.infrastructure.building_dataset.DatasetBuilder.cleanString","text":"Clean a string by unescape html char and tags","title":"cleanString()"},{"location":"infrastructure/building_dataset-reference.html#parameters","text":"text : str str to clean","title":"Parameters"},{"location":"infrastructure/building_dataset-reference.html#returns","text":"str Cleaned str Source code in amendements_analysis/infrastructure/building_dataset.py @staticmethod def cleanString ( text ): \"\"\"Clean a string by unescape html char and tags Parameters ---------- text : str str to clean Returns ------- str Cleaned str \"\"\" text_unescaped = html . unescape ( text ) p = re . compile ( r \"<.*?>\" ) return p . sub ( \"\" , text_unescaped )","title":"Returns"},{"location":"infrastructure/building_dataset-reference.html#amendements_analysis.infrastructure.building_dataset.DatasetBuilder.create_train_test_files","text":"Split dataframe into train and test to write it in order to read it with the transformer","title":"create_train_test_files()"},{"location":"infrastructure/building_dataset-reference.html#parameters","text":"df : pandas.DataFrame Dataframe we want to split","title":"Parameters"},{"location":"infrastructure/building_dataset-reference.html#returns","text":"pandas.DataFrame Two new dataframes got by splitting df Source code in amendements_analysis/infrastructure/building_dataset.py def create_train_test_files ( self , df , force_write_csv = False ): \"\"\"Split dataframe into train and test to write it in order to read it with the transformer Parameters ---------- df : pandas.DataFrame Dataframe we want to split Returns ------- pandas.DataFrame Two new dataframes got by splitting df \"\"\" train_filename = os . path . join ( stg . INTERIM_DIR , stg . CSV_DATA_TRAIN_LM ) test_filename = os . path . join ( stg . INTERIM_DIR , stg . CSV_DATA_TEST_LM ) if force_write_csv or not self . check_files_exists ( train_filename , test_filename ): dataset = df [ \"expose_sommaire\" ] if isinstance ( df , pd . DataFrame ) else df X_train , X_test = train_test_split ( dataset , test_size = 0.2 , random_state = stg . RANDOM_STATE ) X_train . to_csv ( train_filename , index = False ) X_test . to_csv ( test_filename , index = False ) print ( f \"Files X_train & X_test wrote in { stg . INTERIM_DIR } \" ) else : X_train = pd . read_csv ( train_filename ) X_test = pd . read_csv ( test_filename ) return X_train , X_test","title":"Returns"},{"location":"infrastructure/building_dataset-reference.html#amendements_analysis.infrastructure.building_dataset.DatasetBuilder.split_text_into_sentences","text":"Method to split a text into sentences","title":"split_text_into_sentences()"},{"location":"infrastructure/building_dataset-reference.html#parameters","text":"data : str text to split","title":"Parameters"},{"location":"infrastructure/building_dataset-reference.html#returns","text":"list list of sentences got by the split Source code in amendements_analysis/infrastructure/building_dataset.py @staticmethod def split_text_into_sentences ( data ): \"\"\"Method to split a text into sentences Parameters ---------- data : str text to split Returns ------- list list of sentences got by the split \"\"\" reg = re . compile ( r \"(?<!\\b[M]|\\b[Dr])[.?!]\\s*(?=[A-Z])\" ) return reg . split ( data )","title":"Returns"}]}